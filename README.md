# ğŸš€ Projet Data Pipeline Temps RÃ©el + Batch + Orchestration

## ğŸ—ï¸ Architecture

- **Kafka** : Streaming temps rÃ©el des commandes
- **Producer** : GÃ©nÃ¨re des commandes alÃ©atoirement  
- **Consumer** : Consomme et stocke en base
- **PostgreSQL** : Base de donnÃ©es des commandes
- **Airflow** : Orchestration des tÃ¢ches batch
- **Spark** : Traitement big data
- **Redis** : Cache et broker pour Airflow

## ğŸš€ DÃ©marrage rapide

```bash
# Windows
start.bat

# Linux/Mac  
chmod +x start.sh && ./start.sh
```

## ğŸŒ Interfaces

- Airflow: http://localhost:8080 (admin/admin)
- Spark UI: http://localhost:8081

## ğŸ“‹ Commandes utiles

```bash
# Voir les logs
docker logs producer
docker logs consumer  
docker logs kafka

# RedÃ©marrer un service
docker-compose restart producer

# ArrÃªter tout
docker-compose down
```

## ğŸ”§ Configuration PostgreSQL

CrÃ©ez d'abord la base `orders_db` dans votre PostgreSQL local, puis lancez le pipeline.
