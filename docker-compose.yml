version: '3.8'

services:
  # ========================
  # Zookeeper
  # ========================
  zookeeper:
    image: bitnami/zookeeper:latest
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ALLOW_ANONYMOUS_LOGIN: "yes"
    networks:
      - project_net

  # ========================
  # Kafka
  # ========================
  kafka:
    image: bitnami/kafka:3.6.0
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_CFG_NODE_ID: 0
      KAFKA_CFG_PROCESS_ROLES: controller,broker
      KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: 0@kafka:9094
      KAFKA_CFG_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9094
      KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CFG_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CFG_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_KRAFT_CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk
      ALLOW_PLAINTEXT_LISTENER: "yes"
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true"
    depends_on:
      - zookeeper
    networks:
      - project_net

  # ========================
  # Postgres
  # ========================
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: root
      POSTGRES_DB: project
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - project_net

  # ========================
  # Redis
  # ========================
  redis:
    image: redis:alpine
    container_name: redis
    ports:
      - "6379:6379"
    networks:
      - project_net

  # ========================
  # Airflow
  # ========================
  airflow-init:
    image: apache/airflow:2.8.1
    container_name: airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db init
        airflow users create \
          --username admin \
          --password admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com || true
    volumes:
      - ./dags:/opt/airflow/dags
      - ./reports:/opt/airflow/reports
      - airflow_data:/opt/airflow
    networks:
      - project_net

  airflow-webserver:
    image: apache/airflow:2.8.1
    container_name: airflow-webserver
    restart: always
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./reports:/opt/airflow/reports
      - airflow_data:/opt/airflow
      - ./requirements.txt:/requirements.txt
    command: bash -c "pip install --no-cache-dir -r /requirements.txt && airflow webserver"
    networks:
      - project_net

  airflow-scheduler:
    image: apache/airflow:2.8.1
    container_name: airflow-scheduler
    restart: always
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
    volumes:
      - ./dags:/opt/airflow/dags
      - ./reports:/opt/airflow/reports
      - airflow_data:/opt/airflow
      - ./requirements.txt:/requirements.txt
    command: bash -c "pip install --no-cache-dir -r /requirements.txt && airflow scheduler"
    networks:
      - project_net

  # ========================
  # Spark Master
  # ========================
  spark:
    image: bitnami/spark:3.5
    container_name: spark
    environment:
      SPARK_MODE: master
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: "no"
      SPARK_SSL_ENABLED: "no"
    ports:
      - "7077:7077"
      - "8081:8081"
    networks:
      - project_net

  # ========================
  # Spark Worker
  # ========================
  spark-worker:
    image: bitnami/spark:3.5
    container_name: spark-worker
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark:7077
      SPARK_WORKER_MEMORY: 2g
      SPARK_WORKER_CORES: 2
    depends_on:
      - spark
    networks:
      - project_net

  # ========================
  # Jobspark (ton job Spark)
  # ========================
  jobspark:
    build: ./jobspark
    container_name: jobspark
    depends_on:
      - spark
      - spark-worker
      - kafka
      - postgres
    restart: "no"
    networks:
      - project_net

  # ========================
  # Producer / Consumer
  # ========================
  producer:
    build: ./producer
    container_name: producer
    depends_on:
      - kafka
    restart: on-failure
    networks:
      - project_net

  consumer:
    build: ./consumer
    container_name: consumer
    depends_on:
      - kafka
    restart: on-failure
    networks:
      - project_net

  # ========================
  # Jenkins
  # ========================
  jenkins:
    image: jenkins/jenkins:lts
    container_name: jenkins
    user: root
    ports:
      - "8082:8080"
      - "50000:50000"
    volumes:
      - jenkins_home:/var/jenkins_home
    networks:
      - project_net

volumes:
  airflow_data:
  postgres_data:
  jenkins_home:

networks:
  project_net:
    driver: bridge




































































# version: '3.8'

# services:
#   # ========================
#   # Zookeeper
#   # ========================
#   zookeeper:
#     image: bitnami/zookeeper:latest
#     container_name: zookeeper
#     ports:
#       - "2181:2181"
#     environment:
#       ALLOW_ANONYMOUS_LOGIN: "yes"
#     networks:
#       - project_net

#   # ========================
#   # Kafka
#   # ========================
#   kafka:
#     image: bitnami/kafka:3.6.0
#     container_name: kafka
#     ports:
#       - "9092:9092"
#     environment:
#       KAFKA_CFG_NODE_ID: 0
#       KAFKA_CFG_PROCESS_ROLES: controller,broker
#       KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: 0@kafka:9094
#       KAFKA_CFG_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9094
#       KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
#       KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
#       KAFKA_CFG_CONTROLLER_LISTENER_NAMES: CONTROLLER
#       KAFKA_CFG_INTER_BROKER_LISTENER_NAME: PLAINTEXT
#       KAFKA_KRAFT_CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk
#       ALLOW_PLAINTEXT_LISTENER: "yes"
#       KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true"
#     depends_on:
#       - zookeeper
#     networks:
#       - project_net

#   # ========================
#   # Postgres
#   # ========================
#   postgres:
#     image: postgres:13
#     container_name: postgres
#     environment:
#       POSTGRES_USER: postgres
#       POSTGRES_PASSWORD: root
#       POSTGRES_DB: project
#     ports:
#       - "5432:5432"
#     volumes:
#       - postgres_data:/var/lib/postgresql/data
#     networks:
#       - project_net

#   # ========================
#   # Redis
#   # ========================
#   redis:
#     image: redis:alpine
#     container_name: redis
#     ports:
#       - "6379:6379"
#     networks:
#       - project_net

#   # ========================
#   # Airflow
#   # ========================
#   airflow-init:
#     image: apache/airflow:2.8.1
#     container_name: airflow-init
#     entrypoint: /bin/bash
#     command:
#       - -c
#       - |
#         airflow db init
#         airflow users create \
#           --username admin \
#           --password admin \
#           --firstname Admin \
#           --lastname User \
#           --role Admin \
#           --email admin@example.com || true
#     volumes:
#       - ./dags:/opt/airflow/dags
#       - ./reports:/opt/airflow/reports
#       - airflow_data:/opt/airflow
#     networks:
#       - project_net

#   airflow-webserver:
#     image: apache/airflow:2.8.1
#     container_name: airflow-webserver
#     restart: always
#     depends_on:
#       - airflow-init
#     environment:
#       AIRFLOW__CORE__EXECUTOR: SequentialExecutor
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
#       AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
#       AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
#     ports:
#       - "8080:8080"
#     volumes:
#       - ./dags:/opt/airflow/dags
#       - ./reports:/opt/airflow/reports
#       - airflow_data:/opt/airflow
#       - ./requirements.txt:/requirements.txt
#     command: bash -c "pip install --no-cache-dir -r /requirements.txt && airflow webserver"
#     networks:
#       - project_net

#   airflow-scheduler:
#     image: apache/airflow:2.8.1
#     container_name: airflow-scheduler
#     restart: always
#     depends_on:
#       - airflow-init
#     environment:
#       AIRFLOW__CORE__EXECUTOR: SequentialExecutor
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
#     volumes:
#       - ./dags:/opt/airflow/dags
#       - ./reports:/opt/airflow/reports
#       - airflow_data:/opt/airflow
#       - ./requirements.txt:/requirements.txt
#     command: bash -c "pip install --no-cache-dir -r /requirements.txt && airflow scheduler"
#     networks:
#       - project_net

#   # ========================
#   # Spark
#   # ========================
#   spark:
#     image: bitnami/spark:3.5
#     container_name: spark
#     ports:
#       - "7077:7077"
#     networks:
#       - project_net

#   # ========================
#   # Jobspark (ton job Spark)
#   # ========================
#   jobspark:
#     build: ./jobspark
#     container_name: jobspark
#     depends_on:
#       - spark
#       - kafka
#       - postgres
#     restart: "no"
#     networks:
#       - project_net

#   # ========================
#   # Producer / Consumer
#   # ========================
#   producer:
#     build: ./producer
#     container_name: producer
#     depends_on:
#       - kafka
#     restart: on-failure
#     networks:
#       - project_net

#   consumer:
#     build: ./consumer
#     container_name: consumer
#     depends_on:
#       - kafka
#     restart: on-failure
#     networks:
#       - project_net

#   # ========================
#   # Jenkins
#   # ========================
#   jenkins:
#     image: jenkins/jenkins:lts
#     container_name: jenkins
#     user: root
#     ports:
#       - "8082:8080"
#       - "50000:50000"
#     volumes:
#       - jenkins_home:/var/jenkins_home
#     networks:
#       - project_net

# volumes:
#   airflow_data:
#   postgres_data:
#   jenkins_home:

# networks:
#   project_net:
#     driver: bridge







  #kakafta qui marche tres bien  mais le pprobleme cest la 'e variable qui est en local'
  # kafka:
  #   image: bitnami/kafka:3.6.0
  #   container_name: kafka
  #   ports:
  #     - "9092:9092"
  #   environment:
  #     KAFKA_CFG_NODE_ID: 0
  #     KAFKA_CFG_PROCESS_ROLES: controller,broker
  #     KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: 0@kafka:9094
  #     KAFKA_CFG_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9094
  #     KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
  #     KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
  #     KAFKA_CFG_CONTROLLER_LISTENER_NAMES: CONTROLLER
  #     KAFKA_CFG_INTER_BROKER_LISTENER_NAME: PLAINTEXT
  #     KAFKA_KRAFT_CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk
  #     ALLOW_PLAINTEXT_LISTENER: "yes"
  #     KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true"
